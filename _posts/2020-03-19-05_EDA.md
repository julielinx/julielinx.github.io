---
title: "Entry 5: Explore the Data"
categories:
  - Blog
tags:
  - process
---

Now that I have the dataset I put together in [Entry 4](https://julielinx.github.io/blog/get_data/), it's time to see what's in it.

## The Problem

This is the time to get a feel for the data. The kinds of things I'm looking at are:

- Are the variables structured, unstructured, numerical, categorical, etc?
- Are there missing values?
- What are the distributions?
- Are there outliers or noise?
- What's the target variable and which are the features?
- How do the features and target all relate to each other?
- Which features are possibly useful for predicting the target?
- Are there any transformations that would be useful?
  - Square
  - Square root
  - Logarithmic
  - Normalization
  - Standardization
- Is any supplementary data needed?

## The Options

There are a lot of ways to explore data. Descriptive Statistics and visualization are the two most common methods. Looking at the data in a chart can reveal trends that are invisible upon inspection of the descriptive statistics (as demonstrated by [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)). But descriptive statistics also have their place.

<table>
    <tr>
        <td><b>Descriptive statisics include:</b>
            <ul>
                <li>Max</li>
                <li>Min</li>
                <li>Range</li>
                <li>Mean</li>
                <li>Median</li>
                <li>Mode</li>
                <li>Standard deviation</li>
                <li>Variance</li>
                <li>Quartiles</li>
            </ul>
        </td>
        <td><b>Visualization options include:</b>
            <ul>
                <li>Bar charts</li>
                <li> Line charts</li>
                <li>Area charts</li>
                <li>Scatter plots</li>
                <li>Bubble plots</li>
                <li>Box and violin plots</li>
                <li>Heatmaps</li>
                <li>Treemaps</li>
                <li>Histographs and density plots</li>
            </ul>
        </td>
    </tr>
</table>

## The Proposed Solution

The Jupyter notebook with the full code and visuals for my analysis are all in the [Entry 5 notebook](https://github.com/julielinx/datascience_diaries/blob/master/01_ml_process/05_nb_EDA_viz.ipynb).

#### Look at non-null entries

First I used `.info()` to look at the count of non-null entries. A non-null count lower than the number of entries indicates missing values. Missing values were one of those things I talked about in [Entry 2](https://julielinx.github.io/blog/define_process/). They cause some machine learning algorithms to choke.

The planets dataset has no missing values. Of course, I curated this *very* small dataset by hand and made sure there were no missing values. I'll deal with missing values and the different ways to deal with it in <font color="purple">Entry 12</font>.

#### Review variable types

The `.info()` method also gives information about the data type. The reason I like looking at this is just because I think it's one kind of variable doesn't necessarily mean it loaded that way. I've had a numeric column load as a string because a value like 'None' was entered. Double checking small things like this can save a lot of time further down the line.

There are 3 non-numeric columns: type, rings, and magnetic_field. The rest are numeric ([float or int](https://realpython.com/python-data-types/)).

#### Descriptive Statistics

Next I take a glance at the descriptive statistics. The fastest way to get some basic descriptive statistics is to use `.describe()`.  This also shows the count, but includes other useful information like the mean, standard deviation, min, max, and the 25/50/75 quartiles for numeric data. This information is useful to determine what kind of data pre-processing will be needed. Pre-processing will be covered in multiple future entries, but some examples include:

- Scaling
- Normalization
- Selection
- Transformations
- Aggregation
- Cleaning

#### Visualizations

I started with a pairplot of all the numeric data (hint, `pairplot` generally only plots the numeric values). That took several minutes to run (the more variables there are the longer it takes to generate the plot) and gave me a 23 x 23 gridplot.

That's a lot of plots. They were tiny and the text was so small it was unreadable.

I manually narrowed down the features to just what seemed unique and relevant. That left me with eight features, which gave me an 8x8 grid (64 plots). From these, I created iterations of different combinations of features, but mainly focused on how the features related to atmospheric mass.

#### Disclaimer

This problem is interesting in that I'm interested in atmospheric mass, but I'll be predicting a range of planetary mass based on set values of atmospheric mass. As such, atmospheric mass is my variable of interest, but planetary mass is my target value. Generally, the variable of interest is the one that will be the one that's being predicted on.

## The Fail

Whittling down features by hand was useful in this case, but doesn't necessarily (or at all) sound like a task I want to do for the nearly 600 features at work. Nor do I want to do this for every [Kaggle competition](https://www.kaggle.com/competitions) or [UCI dataset](https://archive.ics.uci.edu/ml/datasets.php). On top of that, it's easy to get carried away with visualizations and end up with tens of plots that may or may not say anything about pertinent relationships.

For this 11 observation dataset, I ended up with around 22 plots (I counted each pairplot as a single plot instead of including every plot in the grid as its own plot). And that number includes having narrowed my focus to 5-8 features after the initial look at the pairplots.

Exploring the data is important and can teach me things I didn't know about the data, but what I need is a way to automatically determine feature importance, then concentrate on those features.

#### Side note

When working on a dataset with a large number of features, like the one I have at work, there is usually feature engineering that goes into creating the features. A lot of exploratory analysis goes into this process. That is probably where I would expect EDA to occur. Feature engineering is a topic for another series of entries, but as an introduction some examples of feature engineering include:

- Counts
- Ratios
- Similarities
- Mathematical transformations
  - Log
  - Square
  - Exponentiation
  - Etc
- Aggregations
- NLP
  - Things as simple as character counts
  - Things as complicated as [n-grams](https://en.wikipedia.org/wiki/N-gram)

At this stage in a production pipeline, I'm most interested in how to rank the features and concentrate on just the useful ones.

Enter my fantastic co-worker [Sabber](https://medium.com/@sabber). He proposed using correlation to determine important features and narrow the focus.

## Next Up

[Correlation](https://julielinx.github.io/blog/EDA_correlation/)

## Resources

- [From Data to Viz](https://www.data-to-viz.com/)
- [Data Visualization Society](https://www.datavisualizationsociety.com/challenge)
- [Makeover Monday](https://www.makeovermonday.co.uk/)
- [Seaborn](https://seaborn.pydata.org/tutorial.html)
- [Bokeh](https://docs.bokeh.org/en/latest/index.html)
- [Matplotlib](https://matplotlib.org/contents.html)
- [Plotly](https://plot.ly/python/)