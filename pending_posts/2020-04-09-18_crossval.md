---
title: "Entry 18: Cross-validation"
categories:
  - Blog
tags:
  - model-eval
---


## The Problem

In the last entry, I decided I want to use a hybrid validation method. I intend to split out a hold-out set, then perform cross-validation on the training set. Scikit-Learn has a great chart of this [hybrid method](https://scikit-learn.org/stable/modules/cross_validation.html):

<img src="https://scikit-learn.org/stable/_images/grid_search_cross_validation.png" width=400>

## The Options

The Problem statement may seem like it belongs in The Proposed Solution section, however, there are a lot of variations on cross-validation. The major variants include:

- K-fold
- Leave one out
- Shuffle-split
- Repeated

### K-fold cross-validation

The data is broken into *k* pieces (where *k* is a user specified number, usually five or ten). The model is trained on all *k* pieces except one, then tested on the final piece. This is repeated *k* times, at which point each of the *k* pieces has been used as the test set.

The below images is an example of 5-fold cross-validation:

<img src="../img/k_fold_cv.png" width=500>

- **Split**: this is the row. A split contains all data within the full training set. The split is then broken into folds, or pieces.
- **Fold**: this is the column. A fold holds the same set of observations from one split to the next. What changes is whether that particular fold is allocated for training or testing. Every fold is approximately porportional to every other fold.

An *[An Introduction to Statistical Learning](https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)* says this about the choice of the size of *k*: 'there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation. Typically, given these considerations, one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.'

Once all the k-fold models have been run and tested against their specific test fold, the performance of all models is averaged to produce an overall score.

When using this method, it is important to separate the training and test sets before any preprocessing. If preprocessing is done first, there will be *data leakage*. Data leakage happens when information the model wouldn't have at the time of prediction has been included in the training data. For example, in <font color='red'>Entry 8</font> I covered centering and scaling. Including the values from the test set would alter the mean which is used to center the values. This gives the model information it wouldn't have had if preprocessing was done on only the training set.

When using k-fold cross-validation for classification, it's best to stratify the classes so that the portion of each class is the same in each split as it is in the overall dataset. I covered stratification in <fon color='red'>Entry 17</font>.

The library that was built to accompany *[Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413)*, mglearn, has a good visual representation of this:


```python
import mglearn
mglearn.plots.plot_cross_validation()
```


![png](output_3_0.png)


### Leave-one-out cross-validation

Leave-one-out is very similar to k-fold. It works on the same principle, but only holds out a single observation as the test set. As such, it creates the same number of models as the number of observations.

This method can get computationally expensive really quickly. If the dataset is only as big as some of the toy datasets I've been using as examples, it should be fine. However, the training data for my day job regularly includes hundreds of thousands of observations, if not over a million (depending on the time range we train on).

Leave-one-out can easily be adapted to leave-k-out, where k is a user defined number of observations to include in the test set. This will reduce the computational expence. However, for the large datasets I'm practicing for, I'm leaning toward k-fold cross-validation.

### Shuffle-split cross-validation

This method is similar to a interative/repeated version of the hold-out method. The data is shuffled, then split into train and test sets. This is repeated a user specified number of times (called splits, just like with k-fold cross-validation). The difference between this and k-fold is that, as the name implies, in k-fold the data is broken into folds. Each observation is in one and only one fold. With the shuffle-split method each observation is chosen at random from the full dataset to be part of either the train set or the test set.

In Scikit-Learn there is an option to set the how much data is used for each train and test. This value can be set as an absolute number (specify exactly how many observations should fall in each, ex: 122, 13) or as a floating point number (specify the portion of the full dataset to use, ex: 0.8, 0.2). The numbers can be set to a value below the full dataset, allowing data to be left at each split.

The mglearn library has a good visual representation of this:


```python
mglearn.plots.plot_shuffle_split()
```


![png](output_5_0.png)


This method sounds interesting, however, it is more difficult to use to assess model efficacy the way I want to. Each observation could get one, none, or multiple predictions for it depending on how often it ends up in the test set. This makes it more difficult to run the metrics listed in <fon color='red'>Entry 16</font>.

### Repeated cross-validation

Repeated cross-validation is exactly what it sounds like: repeat a cross-validation technique a user specified number of times. The key here is that each repeatition has different folds. Between one iteration and the next, the data is shuffled and observations are put in different folds before the cross-validation is conducted again.

Supposedly this give a more robust model assessment score, but at the current stage, it sounds like overkill. I may try this at some point in the future to see if it is any more accurate.

## The Proposed Solution

Based on the definitions, challenges, and benefits of the four methods described above, my inclination is to use stratified k-fold cross-validation. *An Introduction to Statistical Learning* sites five or ten folds as having been empirically shown to 'yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.'

K-fold is computationally less expensive than leave-one-out, easier to turn into mathematical metrics than shuffle-split, and more straight forward than repeated cross-validation.



## The Fail

After all the research on cross-validation (this is the third post about it after all) I assumed implementing it would be the easy part.

## Up Next

### Resources

- [Difference in KFold and ShuffleSplit output](https://stackoverflow.com/questions/34731421/whats-the-difference-between-kfold-and-shufflesplit-cv)
- [3.1.2 Cross validation iterators](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators)
- [An Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413/ref=sr_1_15?keywords=scikit+learn&qid=1583195970&s=books&sr=1-15)
- [An Introduction to Statistical Learning](https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics-ebook/dp/B01IBM7790/ref=sr_1_1?crid=3CY4L1LM1MWL3&keywords=an+introduction+to+statistical+learning&qid=1583453336&s=digital-text&sprefix=an+introduction+to+statist%2Cdigital-text%2C156&sr=1-1)
- [A Gentle Introduction to k-fold Cross-Validation](https://machinelearningmastery.com/k-fold-cross-validation/)
- [Why use repeated cross-validation](https://www.kaggle.com/sinanhersek/why-use-repeated-cross-validation)
- [3.1. Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html)


```python

```
